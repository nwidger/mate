\chapter{RESULTS}
\label{RESULTS}

Evaluation of DMP implemented in the maTe virtual machine was
performed by analyzing its performance on a number of benchmark
programs and comparing it to an implementation of the maTe virtual
machine without DMP.

The following three benchmarks were implemented:

\begin{itemize}
\item parallel radix sort - Multithreaded radix sort

\item jacobi - uses the Jacobi method to simulate temperature changes
  on a 20x25 plate.

\item parallel DPLL - Multithreaded boolean satisfiability using DPLL
  algorithm
\end{itemize}

Comparison of my results with those from the DMP/CoreDet papers will
not be a true ``apples-to-apples'' comparison.  The implementations of
the benchmarks used by DMP \cite{dmp} and CoreDet \cite{coredet} were
implemented using highly-tuned C/C++ code whereas for my thesis I
implemented benchmarks in maTe.  However, my thesis is that DMP
executed in a virtual machine can be done with lower overhead.
Overhead is relative to the particular execution environment being
used, therefore the success of my thesis rests on showing that the
relative performance penalty of a maTe program with DMP on is less
than that of executing a C/C++ program with DMP on.

All three benchmarks were implemented in maTe and run across a range
of parameters.  The parameters were:

\begin{itemize}
\item threads - 2, 4, 8 or 16 threads

\item quantum size - 1000, 10000, and 100000 instructions

\item full serial mode or reduced serial mode

\item ownership table granularity - 1, 5 and 10 depth
\end{itemize}

The $Jacobi$ benchmark uses a fixed number of threads, and so this
benchmark does not have a threads parameter.

Before running any benchmarks, I tested my implementation of DMP to
ensure it was truly deterministic by running Racey \cite{racey}, a
deterministic stress test, $10,000$ times for each configuration.  The
results gathered from running the benchmarks will be compared using
the following metrics:

\begin{itemize}
\item overhead - measure difference in execution time when compared to
  a non-DMP virtual machine.

\item sensitivity - measure difference in performance when parameters
  are changed.
\end{itemize}

Each benchmark will be run 10 times for each combination of
parameters.  Run-times shown are averages.

\subsection{Benchmark Results}

\subsubsection{Notation}

In order to make discussing specific benchmark results more concise, I
will use the following notation:

\begin{itemize}
\item $-tN$ - the number of threads used.

\item $-QN$ - the quantum size ($-Q1000$ means a quantum size of
  $1,000$).

\item $-gN$ - the ownership table granularity ($-g5$ means an
  ownership table granularity of $5$).

\item -r - reduced serial mode, the absense of $-r$ means full serial
  mode.
\end{itemize}

For example, $-t2 -Q10000 -g1$ means a run using 2 threads with a
quantum size of $10,000$, an ownership table granularity of $5$ and
full serial mode, whereas $-t8 -Q1000 -g10 -r$ means a run using 8
threads with a quantum size of $1,000$, an ownership table granularity
of $10$ and reduced serial mode.

\subsubsection{Radix}

This benchmark performs a parallel radix search implemented by
repeatedly running counting sort starting with an 8 bit mask.  The
input array contained 500 16-bit random integers between the values of
103 and 65326.  The standard radix search algorithm was parallelized
in two places:

\begin{itemize}
\item Determining the maximum - evenly distribute numbers to N Maxer
  threads.  Each thread iterates over its subset of the shared array
  and determines the largest value.  After, the largest of each
  thread's maximum is determined by iterating serially over each
  Maxer's maximum with the largest used as the global maximum.

\item Masking each number - evenly distribute numbers to N Counter
  threads.  Each thread iterates over its subset of the shared array,
  masking off each number and placing the result in new array.
\end{itemize}

Neither of these segments requires synchronization as the threads are
working on disjoint portions of the array.  Note that arrays are
implemented using Table instances where the keys are Integers.  There
are no synchronized blocks in the maTe source code of this benchmark.

After the Counters are finished, the masked numbers are aggregated
into a single array.  From this point on the counting sort is finished
in a serial fashion.

The number of threads can be given as a command-line option.

\pagebreak

\begin{center}
\begin{small}
\begin{longtable}{llrrlrrr}
\hline
dmp & class & threads & quantum & serial & depth & avg & overhead\\
\hline
nondmp & radix & 2 & - & - & - & 1.95 & .00\\
dmp & radix & 2 & 1000 & full & 1 & 3.06 & .56\\
dmp & radix & 2 & 1000 & full & 5 & 20.74 & 9.63\\
dmp & radix & 2 & 1000 & full & 10 & 44.06 & 21.59\\
dmp & radix & 2 & 1000 & reduced & 1 & 3.06 & .56\\
dmp & radix & 2 & 1000 & reduced & 5 & 20.75 & 9.64\\
dmp & radix & 2 & 1000 & reduced & 10 & 44.07 & 21.60\\
dmp & radix & 2 & 10000 & full & 1 & 3.04 & .55\\
dmp & radix & 2 & 10000 & full & 5 & 6.44 & 2.30\\
dmp & radix & 2 & 10000 & full & 10 & 11.45 & 4.87\\
dmp & radix & 2 & 10000 & reduced & 1 & 3.04 & .55\\
dmp & radix & 2 & 10000 & reduced & 5 & 6.45 & 2.30\\
dmp & radix & 2 & 10000 & reduced & 10 & 11.43 & 4.86\\
dmp & radix & 2 & 100000 & full & 1 & 3.01 & .54\\
dmp & radix & 2 & 100000 & full & 5 & 3.53 & .81\\
dmp & radix & 2 & 100000 & full & 10 & 4.77 & 1.44\\
dmp & radix & 2 & 100000 & reduced & 1 & 3.01 & .54\\
dmp & radix & 2 & 100000 & reduced & 5 & 3.54 & .81\\
dmp & radix & 2 & 100000 & reduced & 10 & 4.77 & 1.44\\
\hline
nondmp & radix & 4 & - & - & - & 1.48 & .00\\
dmp & radix & 4 & 1000 & full & 1 & 2.80 & .89\\
dmp & radix & 4 & 1000 & full & 5 & 24.40 & 15.48\\
dmp & radix & 4 & 1000 & full & 10 & 53.11 & 34.88\\
dmp & radix & 4 & 1000 & reduced & 1 & 2.80 & .89\\
dmp & radix & 4 & 1000 & reduced & 5 & 24.40 & 15.48\\
dmp & radix & 4 & 1000 & reduced & 10 & 53.14 & 34.90\\
dmp & radix & 4 & 10000 & full & 1 & 3.02 & 1.04\\
dmp & radix & 4 & 10000 & full & 5 & 8.35 & 4.64\\
dmp & radix & 4 & 10000 & full & 10 & 16.37 & 10.06\\
dmp & radix & 4 & 10000 & reduced & 1 & 3.03 & 1.04\\
dmp & radix & 4 & 10000 & reduced & 5 & 8.35 & 4.64\\
dmp & radix & 4 & 10000 & reduced & 10 & 16.39 & 10.07\\
dmp & radix & 4 & 100000 & full & 1 & 3.03 & 1.04\\
dmp & radix & 4 & 100000 & full & 5 & 3.97 & 1.68\\
dmp & radix & 4 & 100000 & full & 10 & 6.42 & 3.33\\
dmp & radix & 4 & 100000 & reduced & 1 & 3.03 & 1.04\\
dmp & radix & 4 & 100000 & reduced & 5 & 3.97 & 1.68\\
dmp & radix & 4 & 100000 & reduced & 10 & 6.42 & 3.33\\
\hline
nondmp & radix & 8 & - & - & - & 1.27 & .00\\
dmp & radix & 8 & 1000 & full & 1 & 2.65 & 1.08\\
dmp & radix & 8 & 1000 & full & 5 & 26.11 & 19.55\\
dmp & radix & 8 & 1000 & full & 10 & 58.67 & 45.19\\
dmp & radix & 8 & 1000 & reduced & 1 & 2.67 & 1.10\\
dmp & radix & 8 & 1000 & reduced & 5 & 26.11 & 19.55\\
dmp & radix & 8 & 1000 & reduced & 10 & 58.68 & 45.20\\
dmp & radix & 8 & 10000 & full & 1 & 3.05 & 1.40\\
dmp & radix & 8 & 10000 & full & 5 & 9.43 & 6.42\\
dmp & radix & 8 & 10000 & full & 10 & 20.52 & 15.15\\
dmp & radix & 8 & 10000 & reduced & 1 & 3.05 & 1.40\\
dmp & radix & 8 & 10000 & reduced & 5 & 9.43 & 6.42\\
dmp & radix & 8 & 10000 & reduced & 10 & 20.55 & 15.18\\
dmp & radix & 8 & 100000 & full & 1 & 3.07 & 1.41\\
dmp & radix & 8 & 100000 & full & 5 & 4.42 & 2.48\\
dmp & radix & 8 & 100000 & full & 10 & 9.73 & 6.66\\
dmp & radix & 8 & 100000 & reduced & 1 & 3.07 & 1.41\\
dmp & radix & 8 & 100000 & reduced & 5 & 4.42 & 2.48\\
dmp & radix & 8 & 100000 & reduced & 10 & 9.73 & 6.66\\
\hline
nondmp & radix & 16 & - & - & - & 1.57 & .00\\
dmp & radix & 16 & 1000 & full & 1 & 3.13 & .99\\
dmp & radix & 16 & 1000 & full & 5 & 28.27 & 17.00\\
dmp & radix & 16 & 1000 & full & 10 & 65.80 & 40.91\\
dmp & radix & 16 & 1000 & reduced & 1 & 3.13 & .99\\
dmp & radix & 16 & 1000 & reduced & 5 & 28.30 & 17.02\\
dmp & radix & 16 & 1000 & reduced & 10 & 65.78 & 40.89\\
dmp & radix & 16 & 10000 & full & 1 & 3.17 & 1.01\\
dmp & radix & 16 & 10000 & full & 5 & 10.23 & 5.51\\
dmp & radix & 16 & 10000 & full & 10 & 24.99 & 14.91\\
dmp & radix & 16 & 10000 & reduced & 1 & 3.14 & 1.00\\
dmp & radix & 16 & 10000 & reduced & 5 & 10.24 & 5.52\\
dmp & radix & 16 & 10000 & reduced & 10 & 24.98 & 14.91\\
dmp & radix & 16 & 100000 & full & 1 & 3.11 & .98\\
dmp & radix & 16 & 100000 & full & 5 & 4.48 & 1.85\\
dmp & radix & 16 & 100000 & full & 10 & 13.38 & 7.52\\
dmp & radix & 16 & 100000 & reduced & 1 & 3.11 & .98\\
dmp & radix & 16 & 100000 & reduced & 5 & 4.48 & 1.85\\
dmp & radix & 16 & 100000 & reduced & 10 & 13.40 & 7.53\\
\hline
\caption{Radix Benchmark Results}
\label{tab:radix_results}
\end{longtable}
\end{small}
\end{center}

Table \ref{tab:radix_results} shows the results for the Radix
benchmark.  In the results, the non-DMP runs unsurprisingly beat out
all of the DMP runs with overhead ranging from $54\%$ to $4,520\%$.
The fastest DMP run was $-t8 -Q1000 -g1$ with an execution time of
2.65 seconds.

The execution time of this benchmark does not appear sensitive to the
choice of full or reduced serial mode.  This is likely due to the lack
of sychronized blocks in this benchmark.  Without threads
acquiring/releasing object monitors, the lock counting performed in
reduced serial mode is never triggered causing an early end to a
thread's serial segment.

It is also clear from the results that Radix does not benefit from
increasing the ownership table depth.  The runs with a depth of 1 had
an execution time faster than all the others, with runs at a depth of
10 having the worst.  One possibile explanation for this is that the
object graph generated by the benchmark is not very deep.  Since
threads access shared arrays using Tables, a shallow object graph and
a large ownership depth could lead to threads continuously acquiring
ownership of a large percentage of the total object graph during
serial mode.  This situation could cause greatly increase the number
of blocking reads/writes during parallel mode.

At first it would seem this is bourne out by the results, which show
the percentage of execution time spent in serial mode increasing from
$50\%$ with $-Q1000 -g1$ to $95\%$ with $-Q1000 -g5$ up to $97\%$ with
$-Q1000 -g10$.  However, upon closer inspection the total number of
blocking reads/writes actually went down from around $5600$ with
$-Q1000 -g1$ to around $4000$ with $-Q1000 -g5$ and finally around
$3900$ with $-Q1000 -g10$.  I believe the true reason for the slowdown
is simply the time expense of traversing the object graph while
changing an object's owner during serial mode and not an increase in
future blocking reads/writes caused by changing the object's owner.

\subsubsection{Jacobi}

The Jacobi benchmark uses two ``2-dimensional'' global Tables, one for
the original values and another for the final values.  A worker thread
is created for each row in the table, and each worker runs the Jacobi
algorithm on the cells in its assigned row.  The worker threads each
read/write to the shared original/new global Tables, using
synchronized blocks to manage thread contention.  When each thread
finishes it returns the change in temperature.  When all threads have
finished, the maximum change across all threads is stored.  This whole
process is repeated for 20 iterations.

The number of threads is fixed at the number of rows in the input
table, meaning 20 threads for the 20x25 plate used in the benchmark.

\begin{center}
\begin{small}
\begin{longtable}{llrrlrrr}
\hline
dmp & class & threads & quantum & serial & depth & avg & overhead\\
\hline
nondmp & jacobi & 2 & - & - & - & 2.92 & .00\\
dmp & jacobi & 2 & 1000 & full & 1 & 17.22 & 4.89\\
dmp & jacobi & 2 & 1000 & full & 5 & 35.52 & 11.16\\
dmp & jacobi & 2 & 1000 & full & 10 & 35.54 & 11.17\\
dmp & jacobi & 2 & 1000 & reduced & 1 & 11.90 & 3.07\\
dmp & jacobi & 2 & 1000 & reduced & 5 & 24.64 & 7.43\\
dmp & jacobi & 2 & 1000 & reduced & 10 & 24.63 & 7.43\\
dmp & jacobi & 2 & 10000 & full & 1 & 16.00 & 4.47\\
dmp & jacobi & 2 & 10000 & full & 5 & 34.99 & 10.98\\
dmp & jacobi & 2 & 10000 & full & 10 & 35.00 & 10.98\\
dmp & jacobi & 2 & 10000 & reduced & 1 & 3.71 & .27\\
dmp & jacobi & 2 & 10000 & reduced & 5 & 6.98 & 1.39\\
dmp & jacobi & 2 & 10000 & reduced & 10 & 6.97 & 1.38\\
dmp & jacobi & 2 & 100000 & full & 1 & 15.98 & 4.47\\
dmp & jacobi & 2 & 100000 & full & 5 & 35.02 & 10.99\\
dmp & jacobi & 2 & 100000 & full & 10 & 35.03 & 10.99\\
dmp & jacobi & 2 & 100000 & reduced & 1 & 3.72 & .27\\
dmp & jacobi & 2 & 100000 & reduced & 5 & 6.97 & 1.38\\
dmp & jacobi & 2 & 100000 & reduced & 10 & 7.00 & 1.39\\
\hline
\caption{Jacobi Benchmark Results}
\label{tab:jacobi_results}
\end{longtable}
\end{small}
\end{center}

Table \ref{tab:jacobi_results} shows the results for the Jacobi
benchmark.  The non-DMP Jacobi runs averaged 2.91 seconds of runtime.
With DMP enabled, the overhead ranged from $27\%$ to $1,117\%$.  The
best DMP run was $-Q10000 -g1 -r$ with an average time of 3.71
seconds.  The most surprising result from the Jacobi benchmark is the
clear advantage given to the runs using reduced serial mode.  The top
seven non-DMP runs by both execution time and overhead are using
reduced serial mode.  This is likely due to the fact that the main
loop involves each thread acquiring and quickly releasing the old and
new Table's monitors.  The sensitivity to the chosen serial mode on
this benchmark is strong enough to more than quadruple execution time
in some cases (compare $-Q10000 -g1 -r$ and $-Q10000 -g1$).  The
results show that simply switching from reduced to full serial mode
between $-Q10000 -g1 -r$ and $-Q10000 -g1$ increased the total number
of blocking reads/writes from $1842$ to $17,616$, which increased the
total number of rounds from $175$ to $3081$.

\subsubsection{Parallel DPLL}

The DPLL benchmark solves boolean satisfiability problems using a
parallelized form of the Davis-Putnam-Logemann-Loveland (DPLL)
algorithm.  Input files are 3-SAT problems in CNF format.  The
algorithm is parallelized by creating N threads, given at the
command-line, where each thread has a queue of nodes on the truth tree
to process.  The queue is made up of chained buckets, one bucket for
each level on the tree (corresponding to each variable in the input
problem).  At the beginning of the program, the root of the tree is
pushed onto the queue of the first thread.  Each thread continually
removes a node from its queue, simplifies the input using the value of
true for the node's variable and pushes the false node onto its queue.
If a thread reaches a dead-end in its subtree and has no more nodes on
its queue, it steals a node off the queue of a randomly selected
thread and continues.

Each thread must acquire the monitor of a queue's bucket before
adding/removing a node from that level.

\begin{center}
\begin{small}
\begin{longtable}{llrrlrrr}
\hline
dmp & class & threads & quantum & serial & depth & avg & overhead\\
\hline
nondmp & dpl & 2 & - & - & - & .71 & .00\\
dmp & dpll & 2 & 1000 & full & 1 & 1.72 & 1.42\\
dmp & dpll & 2 & 1000 & full & 5 & 1.67 & 1.35\\
dmp & dpll & 2 & 1000 & full & 10 & 1.72 & 1.42\\
dmp & dpll & 2 & 1000 & reduced & 1 & 1.59 & 1.23\\
dmp & dpll & 2 & 1000 & reduced & 5 & 1.85 & 1.60\\
dmp & dpll & 2 & 1000 & reduced & 10 & 2.02 & 1.84\\
dmp & dpll & 2 & 10000 & full & 1 & 1.12 & .57\\
dmp & dpll & 2 & 10000 & full & 5 & 1.00 & .40\\
dmp & dpll & 2 & 10000 & full & 10 & 1.20 & .69\\
dmp & dpll & 2 & 10000 & reduced & 1 & .95 & .33\\
dmp & dpll & 2 & 10000 & reduced & 5 & 1.08 & .52\\
dmp & dpll & 2 & 10000 & reduced & 10 & 1.10 & .54\\
dmp & dpll & 2 & 100000 & full & 1 & 1.05 & .47\\
dmp & dpll & 2 & 100000 & full & 5 & 1.03 & .45\\
dmp & dpll & 2 & 100000 & full & 10 & 1.34 & .88\\
dmp & dpll & 2 & 100000 & reduced & 1 & 1.11 & .56\\
dmp & dpll & 2 & 100000 & reduced & 5 & 1.25 & .76\\
dmp & dpll & 2 & 100000 & reduced & 10 & 1.20 & .69\\
\hline
nondmp & dpll & 4 & - & - & - & 1.93 & .00\\
dmp & dpll & 4 & 1000 & full & 1 & 3.66 & .89\\
dmp & dpll & 4 & 1000 & full & 5 & 3.81 & .97\\
dmp & dpll & 4 & 1000 & full & 10 & 4.35 & 1.25\\
dmp & dpll & 4 & 1000 & reduced & 1 & 3.85 & .99\\
dmp & dpll & 4 & 1000 & reduced & 5 & 3.80 & .96\\
dmp & dpll & 4 & 1000 & reduced & 10 & 4.10 & 1.12\\
dmp & dpll & 4 & 10000 & full & 1 & 2.40 & .24\\
dmp & dpll & 4 & 10000 & full & 5 & 3.04 & .57\\
dmp & dpll & 4 & 10000 & full & 10 & 2.56 & .32\\
dmp & dpll & 4 & 10000 & reduced & 1 & 2.40 & .24\\
dmp & dpll & 4 & 10000 & reduced & 5 & 2.58 & .33\\
dmp & dpll & 4 & 10000 & reduced & 10 & 2.82 & .46\\
dmp & dpll & 4 & 100000 & full & 1 & 2.22 & .15\\
dmp & dpll & 4 & 100000 & full & 5 & 2.30 & .19\\
dmp & dpll & 4 & 100000 & full & 10 & 2.72 & .40\\
dmp & dpll & 4 & 100000 & reduced & 1 & 2.15 & .11\\
dmp & dpll & 4 & 100000 & reduced & 5 & 2.06 & .06\\
dmp & dpll & 4 & 100000 & reduced & 10 & 2.41 & .24\\
\hline
nondmp & dpll & 8 & - & - & - & 4.06 & .00\\
dmp & dpll & 8 & 1000 & full & 1 & 13.15 & 2.23\\
dmp & dpll & 8 & 1000 & full & 5 & 12.88 & 2.17\\
dmp & dpll & 8 & 1000 & full & 10 & 23.28 & 4.73\\
dmp & dpll & 8 & 1000 & reduced & 1 & 11.52 & 1.83\\
dmp & dpll & 8 & 1000 & reduced & 5 & 11.47 & 1.82\\
dmp & dpll & 8 & 1000 & reduced & 10 & 21.76 & 4.35\\
dmp & dpll & 8 & 10000 & full & 1 & 5.44 & .33\\
dmp & dpll & 8 & 10000 & full & 5 & 5.95 & .46\\
dmp & dpll & 8 & 10000 & full & 10 & 7.47 & .83\\
dmp & dpll & 8 & 10000 & reduced & 1 & 5.46 & .34\\
dmp & dpll & 8 & 10000 & reduced & 5 & 5.86 & .44\\
dmp & dpll & 8 & 10000 & reduced & 10 & 7.91 & .94\\
dmp & dpll & 8 & 100000 & full & 1 & 5.55 & .36\\
dmp & dpll & 8 & 100000 & full & 5 & 6.16 & .51\\
dmp & dpll & 8 & 100000 & full & 10 & 6.96 & .71\\
dmp & dpll & 8 & 100000 & reduced & 1 & 3.12 & -.23\\
dmp & dpll & 8 & 100000 & reduced & 5 & 3.45 & -.15\\
dmp & dpll & 8 & 100000 & reduced & 10 & 3.57 & -.12\\
\hline
nondmp & dpll & 16 & - & - & - & 6.12 & .00\\
dmp & dpll & 16 & 1000 & full & 1 & 21.26 & 2.47\\
dmp & dpll & 16 & 1000 & full & 5 & 21.29 & 2.47\\
dmp & dpll & 16 & 1000 & full & 10 & 176.84 & 27.89\\
dmp & dpll & 16 & 1000 & reduced & 1 & 22.31 & 2.64\\
dmp & dpll & 16 & 1000 & reduced & 5 & 20.36 & 2.32\\
dmp & dpll & 16 & 1000 & reduced & 10 & 160.39 & 25.20\\
dmp & dpll & 16 & 10000 & full & 1 & 9.76 & .59\\
dmp & dpll & 16 & 10000 & full & 5 & 11.85 & .93\\
dmp & dpll & 16 & 10000 & full & 10 & 47.47 & 6.75\\
dmp & dpll & 16 & 10000 & reduced & 1 & 8.53 & .39\\
dmp & dpll & 16 & 10000 & reduced & 5 & 9.40 & .53\\
dmp & dpll & 16 & 10000 & reduced & 10 & 33.83 & 4.52\\
dmp & dpll & 16 & 100000 & full & 1 & 11.78 & .92\\
dmp & dpll & 16 & 100000 & full & 5 & 11.22 & .83\\
dmp & dpll & 16 & 100000 & full & 10 & 26.70 & 3.36\\
dmp & dpll & 16 & 100000 & reduced & 1 & 5.14 & -.16\\
dmp & dpll & 16 & 100000 & reduced & 5 & 5.38 & -.12\\
dmp & dpll & 16 & 100000 & reduced & 10 & 10.54 & .72\\
\hline
\caption{DPLL Benchmark Results}
\label{tab:dpll_results}
\end{longtable}
\end{small}
\end{center}

When generating the results for this benchmark, the maTe virtual
machine was run in a special mode which ensures the $Integer.rand$
method used in determining which level of a thread's queue to steal
from always returns the same sequence of numbers.

Table \ref{tab:dpll_results} shows the results for the DPLL benchmark.
The non-DMP DPLL runs averaged .71 seconds (2 threads), 1.93 seconds
(4 threads), 4.06 seconds (8 threads) and 6.12 seconds (16).  With DMP
enabled the overhead ranged from $-23\%$ to $2,789\%$.  The best DMP
run was $-t2 -Q10000 -r -g1$ with an average time of .95 seconds.

The inefficiency of the maTe virtual machine when running multithreads
is very apparent in this benchmark.  The average execution time for
non-DMP runs gets worse as more threads are added.  Furthermore, many
of the DMP runs executed faster than non-DMP runs executed with a
higher thread count.

DMP run of 8 threads with $-Q100000 -r -g1$ had an overhead of $-23\%$
with an average execution time of 3.12 seconds, beating out the
non-DMP run average time with 8 threads of 4.06 seconds.

% Higher instruction quantum values appear to perform better.

For runs with 2 or 4 threads, different ownership table depths had
little effect on execution time, adding no more than .64 seconds.
Runs with 8 and 16 threads, however, exhibit extreme jumps in
execution time when the ownership table depth is increased, compare
the $-t8 -Q1000 -r -g5$ run with an execution time of 11.47 seconds
with the $-t8 -Q1000 -r -g10$ run of 21.76 seconds.  With 16 threads,
this trend gets even worse, with $-t16 -Q1000 -g5$ executing in 21.29
seconds and $-t16 -Q1000 -g10$ executing in 176.84 seconds.

% Overhead is very sensitive to DMP parameters chosen.

% Does not appear sensitive to choice of serial mode.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
